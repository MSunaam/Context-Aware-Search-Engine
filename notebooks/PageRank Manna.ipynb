{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important\n",
        "1. Run the cell below once and restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vypv4BvL1cgW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -qqq install pyspark python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oPbuW3171fxD"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, udf, collect_list, explode, sum as spark_sum\n",
        "from pyspark.sql.types import ArrayType, FloatType, BooleanType, StructType, StructField, StringType\n",
        "import os\n",
        "import json\n",
        "from dotenv import dotenv_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_items([('SPLIT_DATASET_DIR', '/dataset/splitupDataset'), ('LOG_DIR', '/Crawler/logs'), ('OUTPUT_DIR', '/Crawler/output'), ('SCRAPED_DATA', '../Crawler/output')])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = dotenv_values(\"../.env\")\n",
        "config.items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fSVs1BP14CD"
      },
      "source": [
        "**Graph Class**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DR6z6JpH1mDS"
      },
      "outputs": [],
      "source": [
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.graph = {}\n",
        "\n",
        "    def add_edge(self, u, v):\n",
        "        if u in self.graph:\n",
        "            self.graph[u].append(v)\n",
        "        else:\n",
        "            self.graph[u] = [v]\n",
        "    def __str__(self):\n",
        "        result = \"\"\n",
        "        for node, neighbors in self.graph.items():\n",
        "            result += f\"{node}: {neighbors}\\n\"\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwnORbd118qi"
      },
      "source": [
        "**Graph Builder**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "00zMbmYy1ozS"
      },
      "outputs": [],
      "source": [
        "def build_graph_from_json_folder(folder_path):\n",
        "    graph = Graph()\n",
        "    print(\"Graph building started\")\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as file:\n",
        "                data = json.load(file)\n",
        "                page_name = data.get('url')\n",
        "                forward_links = data.get('forwardLinks', [])\n",
        "                if not forward_links:\n",
        "                    forward_links = [None]\n",
        "                for link in forward_links:\n",
        "                    graph.add_edge(page_name, link)\n",
        "    print(\"Graph building ended\")\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJqa5NlA2DNa"
      },
      "source": [
        "**Page Rank**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DeJacehx1q8i"
      },
      "outputs": [],
      "source": [
        "def rank_dist(link_list, rank):\n",
        "    len_link_list = len(link_list)\n",
        "    if len_link_list > 0 and link_list[0] is not None:\n",
        "        rank = rank / len_link_list\n",
        "        r_list = [(x, rank) for x in link_list]\n",
        "    else:\n",
        "        r_list = [(\"DANGLING\", rank)]\n",
        "    return r_list\n",
        "\n",
        "inner_schema = StructType([\n",
        "    StructField(\"uri_id\", StringType(), False),\n",
        "    StructField(\"rank\", FloatType(), False)\n",
        "])\n",
        "\n",
        "ranks_dist_udf = udf(rank_dist, ArrayType(inner_schema))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GruUEf0m1t8r"
      },
      "outputs": [],
      "source": [
        "def run(graph, alpha=0.15, convergence=0.01):\n",
        "    edge_info = [(src, dst) for src, dst_list in graph.graph.items() for dst in dst_list]\n",
        "    spark = SparkSession.builder.appName(\"PageRank\").getOrCreate()\n",
        "    print(\"Start Spark session\")\n",
        "    edges_df = spark.createDataFrame(edge_info, ['src', 'dst'])\n",
        "\n",
        "    dataframe_ranked = edges_df\\\n",
        "        .groupby('src')\\\n",
        "        .agg(collect_list('dst').alias('dst_list'))\\\n",
        "        .withColumnRenamed('src', 'uri_id')\n",
        "\n",
        "    dataframe_ranked = dataframe_ranked.withColumn('rank', lit(1.0 / dataframe_ranked.count()))\n",
        "\n",
        "    n_nodes = dataframe_ranked.count()\n",
        "    i = 0\n",
        "    checkpoint_dir = 'checkpoint_directory'  # Ensure this directory exists\n",
        "    spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
        "    while True:\n",
        "        dataframe_ranked.cache()\n",
        "\n",
        "        ranks_one_df = dataframe_ranked.withColumn(\n",
        "            'link_map_pr', ranks_dist_udf('dst_list', 'rank'))\n",
        "        ranks_one_df = ranks_one_df.select(\n",
        "            explode('link_map_pr').alias('exploded'))\n",
        "        ranks_one_df = ranks_one_df\\\n",
        "            .withColumn('dst_id', ranks_one_df['exploded'].getItem('uri_id'))\\\n",
        "            .withColumn('rank_i', ranks_one_df['exploded'].getItem('rank'))\\\n",
        "            .drop('exploded')\n",
        "\n",
        "        ranks_one_df = ranks_one_df\\\n",
        "            .groupby('dst_id')\\\n",
        "            .sum('rank_i')\\\n",
        "            .withColumnRenamed('sum(rank_i)', 'rank_i')\n",
        "\n",
        "        dataframe_ranked = dataframe_ranked\\\n",
        "            .join(ranks_one_df, dataframe_ranked['uri_id'] == ranks_one_df['dst_id'], 'outer')\\\n",
        "            .drop('dst_id')\n",
        "\n",
        "        dangling_rank = dataframe_ranked\\\n",
        "            .filter(dataframe_ranked.uri_id == \"DANGLING\")\\\n",
        "            .select(spark_sum('rank_i'))\\\n",
        "            .first()[0]\n",
        "\n",
        "        dataframe_ranked = dataframe_ranked.filter(dataframe_ranked.uri_id != \"DANGLING\")\n",
        "\n",
        "        if dangling_rank:\n",
        "            dist_alpha = ((dangling_rank / n_nodes) * (1 - alpha)) + alpha\n",
        "        else:\n",
        "            dist_alpha = alpha\n",
        "\n",
        "        sum_alpha_and_pr_udf = udf(lambda x: (x * (1 - alpha)) + dist_alpha, FloatType())\n",
        "        dataframe_ranked = dataframe_ranked.na.fill(0, ['rank_i'])\n",
        "        dataframe_ranked = dataframe_ranked.withColumn('rank_i', sum_alpha_and_pr_udf('rank_i'))\n",
        "\n",
        "        convergence_udf = udf(lambda rank_i, rank: abs(rank_i - rank) <= convergence, BooleanType())\n",
        "        dataframe_ranked = dataframe_ranked.withColumn('convergence', convergence_udf('rank', 'rank_i'))\n",
        "        count_not_converged = dataframe_ranked.filter(dataframe_ranked.convergence == False).count()\n",
        "        dataframe_ranked = dataframe_ranked.drop('convergence').drop('rank').withColumnRenamed('rank_i', 'rank')\n",
        "\n",
        "        dataframe_ranked = dataframe_ranked.checkpoint()\n",
        "\n",
        "        if count_not_converged == 0:\n",
        "            print(\"All nodes convergend according to the given criteria\")\n",
        "            break\n",
        "        else:\n",
        "            print(f'Nodes not yet converged: {count_not_converged}')\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    output_graph = {}\n",
        "    for row in dataframe_ranked.collect():\n",
        "        output_graph[row['uri_id']] = row['rank']\n",
        "\n",
        "    return output_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viEfJqJq2Jii"
      },
      "source": [
        "**Driver Code**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D08g_15c10qa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/05/26 18:35:06 WARN Utils: Your hostname, Muhammads-MacBook-Pro-4.local resolves to a loopback address: 127.0.0.1; using 10.7.94.91 instead (on interface en0)\n",
            "24/05/26 18:35:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/05/26 18:35:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph building started\n",
            "Graph building ended\n",
            "Start ranking\n",
            "Start Spark session\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/05/26 18:35:23 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nodes not yet converged: 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 60:=====================================================>(199 + 1) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All nodes convergend according to the given criteria\n",
            "PageRank Results:\n",
            "{'https://britannica.com': 0.15000000596046448, 'https://wordpress.com': 0.15000000596046448, 'https://instagram.com': 0.15000000596046448, 'https://ebay.com': 0.15000000596046448, 'https://nih.gov': 0.15000000596046448, 'https://wikipedia.org': 0.15000000596046448, 'https://quora.com': 0.15000000596046448, 'https://sciencedirect.com': 0.15000000596046448, 'https://facebook.com': 0.15000000596046448, 'https://researchgate.net': 0.15000000596046448, 'https://indeed.com': 0.15000000596046448, 'https://etsy.com': 0.15000000596046448, 'https://linkedin.com': 0.15000000596046448, 'https://youtube.com': 0.15000000596046448, 'https://pinterest.com': 0.15000000596046448, 'https://medium.com': 0.15000000596046448}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    spark = SparkSession.builder.appName(\"PageRank\").getOrCreate()\n",
        "    folder_path = config['SCRAPED_DATA']\n",
        "    graph = build_graph_from_json_folder(folder_path)\n",
        "    print(\"Start ranking\")\n",
        "    ranked_graph = run(graph)\n",
        "    print(\"PageRank Results:\")\n",
        "    print(ranked_graph)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
